{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcc286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Core Model Definition and Helper Functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set global parameters\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "learning_rate = 1e-3\n",
    "max_iters = 5000\n",
    "\n",
    "# Define improved language model\n",
    "class ImprovedLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        embeddings = self.embedding(idx)  \n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        logits = self.fc(lstm_out)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens, temperature=0.8):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# Function to get a batch of data\n",
    "def get_batch(split):\n",
    "    # Select appropriate data\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Generate random starting indices\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # Create batch inputs and targets\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    # Move to device\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Function to estimate loss\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(model, data, num_samples=1000, context_len=10):\n",
    "    if len(data) == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Sample random positions\n",
    "    if len(data) <= context_len:\n",
    "        positions = [0]\n",
    "    else:\n",
    "        max_pos = min(len(data) - context_len - 1, num_samples)\n",
    "        if max_pos <= 0:\n",
    "            return 0.0\n",
    "        positions = random.sample(range(max_pos), min(num_samples, max_pos))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for pos in positions:\n",
    "            # Get context and next character\n",
    "            x = data[pos:pos+context_len].unsqueeze(0).to(device)\n",
    "            y_true = data[pos+context_len].to(device)\n",
    "            \n",
    "            # Get model prediction\n",
    "            logits, _ = model(x)\n",
    "            y_pred = torch.argmax(logits[0, -1, :])\n",
    "            \n",
    "            # Compare\n",
    "            if y_pred == y_true:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    model.train()\n",
    "    return (correct / total * 100) if total > 0 else 0.0\n",
    "\n",
    "# Function to load a model\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained model and vocabulary\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    vocab_size = checkpoint['vocab_size']\n",
    "    chars = checkpoint['chars']\n",
    "    string_to_int = checkpoint['string_to_int']\n",
    "    int_to_string = checkpoint['int_to_string']\n",
    "    \n",
    "    # Create and load model\n",
    "    model = ImprovedLanguageModel(vocab_size)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    return model, chars, string_to_int, int_to_string, device\n",
    "\n",
    "# Function to generate text\n",
    "def predict_text(model, input_text, string_to_int, int_to_string, device, num_chars=100, temperature=0.8):\n",
    "    \"\"\"Generate text continuation from input\"\"\"\n",
    "    # Convert input text to tensor\n",
    "    encode = lambda s: [string_to_int.get(c, 0) for c in s]  # Default to 0 if char not found\n",
    "    decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "    \n",
    "    # Handle empty input\n",
    "    if not input_text:\n",
    "        input_tensor = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "    else:\n",
    "        input_tensor = torch.tensor(encode(input_text), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Generate text continuation\n",
    "    with torch.no_grad():\n",
    "        output_tensor = model.generate(input_tensor, max_new_tokens=num_chars, temperature=temperature)\n",
    "    \n",
    "    # Get the full generated text\n",
    "    generated_text = decode(output_tensor[0].tolist())\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Function for the interactive mode\n",
    "def interactive_mode(model, string_to_int, int_to_string, device):\n",
    "    \"\"\"Run interactive prediction mode\"\"\"\n",
    "    print(\"\\n===== Language Model Predictor =====\")\n",
    "    print(\"Type some text and the model will continue it.\")\n",
    "    print(\"Commands:\")\n",
    "    print(\"  :temp X    - Set temperature (0.1-2.0, default 0.8)\")\n",
    "    print(\"  :length X  - Set generation length (default 100)\")\n",
    "    print(\"  :exit      - Quit the program\")\n",
    "    \n",
    "    temperature = 0.8\n",
    "    length = 100\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            input_text = input(\"\\nYour text: \")\n",
    "            \n",
    "            # Handle commands\n",
    "            if input_text.startswith(':'):\n",
    "                cmd = input_text.lower()\n",
    "                if cmd == ':exit':\n",
    "                    break\n",
    "                elif cmd.startswith(':temp '):\n",
    "                    try:\n",
    "                        temperature = float(cmd.split()[1])\n",
    "                        print(f\"Temperature set to {temperature}\")\n",
    "                    except:\n",
    "                        print(\"Invalid temperature value\")\n",
    "                elif cmd.startswith(':length '):\n",
    "                    try:\n",
    "                        length = int(cmd.split()[1])\n",
    "                        print(f\"Length set to {length}\")\n",
    "                    except:\n",
    "                        print(\"Invalid length value\")\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nGenerating text...\")\n",
    "            generated_text = predict_text(model, input_text, string_to_int, int_to_string, \n",
    "                                          device, num_chars=length, temperature=temperature)\n",
    "            \n",
    "            print(\"\\nGenerated text:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(generated_text)\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74db086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading and Training Functions\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Define global variables\n",
    "model = None\n",
    "optimizer = None\n",
    "train_data = None\n",
    "val_data = None\n",
    "string_to_int = None\n",
    "int_to_string = None\n",
    "\n",
    "# Function to load text data\n",
    "def load_text_data(file_path='sample2.txt'):\n",
    "    \"\"\"Load text data from a file and prepare for processing\"\"\"\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "        print(\"Created 'data' directory for text files\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        print(f\"Loaded {len(text)} characters from {file_path}\")\n",
    "        \n",
    "        # Create character mapping\n",
    "        chars = sorted(set(text))\n",
    "        vocabulary_size = len(chars)\n",
    "        print(f\"Vocabulary size: {vocabulary_size} unique characters\")\n",
    "        \n",
    "        # Create encoder/decoder\n",
    "        string_to_int = {c: i for i, c in enumerate(chars)}\n",
    "        int_to_string = {i: c for i, c in enumerate(chars)}\n",
    "        \n",
    "        # Convert to tensor\n",
    "        data = torch.tensor(\n",
    "            [string_to_int[c] for c in text], \n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        return text, chars, string_to_int, int_to_string, data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        if file_path != 'sample2.txt':\n",
    "            print(\"Trying to load default file instead...\")\n",
    "            return load_text_data('sample2.txt')\n",
    "        else:\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to train on a selected data file\n",
    "def train_on_file(file_path='sample2.txt', epochs=20):\n",
    "    \"\"\"Train the language model on a specific text file\"\"\"\n",
    "    global model, optimizer, train_data, val_data, string_to_int, int_to_string\n",
    "    \n",
    "    # Load and prepare data\n",
    "    text, chars, string_to_int, int_to_string, data = load_text_data(file_path)\n",
    "    vocabulary_size = len(chars)\n",
    "    \n",
    "    # Split data\n",
    "    n = int(0.8 * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    print(\"Initializing model...\")\n",
    "    model = ImprovedLanguageModel(vocabulary_size)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Training parameters\n",
    "    iters_per_epoch = max(1, min(500, len(train_data) // (block_size * batch_size)))\n",
    "    total_iters = epochs * iters_per_epoch\n",
    "    eval_interval = max(1, min(100, iters_per_epoch // 2))\n",
    "    \n",
    "    print(f\"Training for {epochs} epochs with {iters_per_epoch} iterations per epoch\")\n",
    "    print(f\"Total iterations: {total_iters}, evaluating every {eval_interval} steps\")\n",
    "    \n",
    "    # Training loop with epochs\n",
    "    print(\"Starting training...\")\n",
    "    global_iter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Track progress within epoch\n",
    "        for local_iter in range(iters_per_epoch):\n",
    "            # Evaluate periodically\n",
    "            if global_iter % eval_interval == 0:\n",
    "                losses = estimate_loss()\n",
    "                # Calculate and report training and validation accuracy\n",
    "                train_accuracy = calculate_accuracy(model, train_data[:min(10000, len(train_data))])\n",
    "                val_accuracy = calculate_accuracy(model, val_data[:min(5000, len(val_data))])\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Step {local_iter+1}/{iters_per_epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "                print(f\"Accuracies: train {train_accuracy:.2f}%, val {val_accuracy:.2f}%\")\n",
    "            \n",
    "            # Get batch and train\n",
    "            try:\n",
    "                xb, yb = get_batch('train')\n",
    "                logits, loss = model(xb, yb)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                continue\n",
    "            \n",
    "            global_iter += 1\n",
    "        \n",
    "        # Evaluate at the end of each epoch\n",
    "        try:\n",
    "            epoch_losses = estimate_loss()\n",
    "            print(f\"End of epoch {epoch+1}/{epochs}: train loss {epoch_losses['train']:.4f}, val loss {epoch_losses['val']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating at epoch end: {e}\")\n",
    "    \n",
    "    # Calculate final accuracy\n",
    "    try:\n",
    "        final_train_accuracy = calculate_accuracy(model, train_data)\n",
    "        final_val_accuracy = calculate_accuracy(model, val_data)\n",
    "        print(f\"\\nFinal accuracy: train {final_train_accuracy:.2f}%, val {final_val_accuracy:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating final accuracy: {e}\")\n",
    "    \n",
    "    # Save the model\n",
    "    try:\n",
    "        save_path = f\"models/language_model_{os.path.basename(file_path).split('.')[0]}.pt\"\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'vocab_size': vocabulary_size,\n",
    "            'chars': chars,\n",
    "            'string_to_int': string_to_int,\n",
    "            'int_to_string': int_to_string\n",
    "        }, save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    return model, chars, string_to_int, int_to_string\n",
    "\n",
    "# Interactive file selection and training\n",
    "def select_and_train():\n",
    "    \"\"\"Interactive menu to select a file and train the model\"\"\"\n",
    "    # Ensure data directory exists\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "        print(\"Created 'data' directory. Please add text files to this directory.\")\n",
    "        return\n",
    "    \n",
    "    # List available files\n",
    "    try:\n",
    "        files = [f for f in os.listdir('data') if f.endswith('.txt')]\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files: {e}\")\n",
    "        files = []\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No text files found in the 'data' directory. Please add some .txt files.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nAvailable text files:\")\n",
    "    for i, file in enumerate(files):\n",
    "        print(f\"{i+1}. {file}\")\n",
    "    \n",
    "    # Get user selection\n",
    "    try:\n",
    "        choice = input(\"\\nSelect a file number (or press Enter for default sample2.txt): \")\n",
    "        if choice.strip() == '':\n",
    "            file_path = 'sample2.txt'\n",
    "        else:\n",
    "            try:\n",
    "                idx = int(choice) - 1\n",
    "                if 0 <= idx < len(files):\n",
    "                    file_path = f\"data/{files[idx]}\"\n",
    "                else:\n",
    "                    print(\"Invalid selection. Using default file.\")\n",
    "                    file_path = 'sample2.txt'\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Using default file.\")\n",
    "                file_path = 'sample2.txt'\n",
    "        \n",
    "        # Get epochs\n",
    "        epochs = input(\"Enter number of epochs (default: 20): \")\n",
    "        epochs = int(epochs) if epochs.strip().isdigit() else 20\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nTraining on {file_path} for {epochs} epochs...\")\n",
    "        model, chars, string_to_int, int_to_string = train_on_file(file_path, epochs)\n",
    "        \n",
    "        # Ask if user wants to enter interactive mode\n",
    "        response = input(\"\\nWould you like to enter interactive mode? (y/n): \")\n",
    "        if response.lower() in ['y', 'yes']:\n",
    "            interactive_mode(model, string_to_int, int_to_string, device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in file selection or training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Project Structure and Main Menu\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def create_project_structure():\n",
    "    \"\"\"Create the project directory structure if it doesn't exist\"\"\"\n",
    "    dirs = ['data', 'models', 'bitnet_cache']\n",
    "    for dir_name in dirs:\n",
    "        try:\n",
    "            if not os.path.exists(dir_name):\n",
    "                os.makedirs(dir_name)\n",
    "                print(f\"Created '{dir_name}' directory\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directory {dir_name}: {e}\")\n",
    "    \n",
    "    # Check if sample2.txt needs to be moved\n",
    "    try:\n",
    "        if os.path.exists('sample2.txt') and not os.path.exists('sample2.txt'):\n",
    "            import shutil\n",
    "            shutil.copy('sample2.txt', 'sample2.txt')\n",
    "            print(\"Moved sample2.txt to data directory\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving sample file: {e}\")\n",
    "\n",
    "def main_menu():\n",
    "    \"\"\"Display the main menu for model selection and training\"\"\"\n",
    "    create_project_structure()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            clear_output(wait=True)\n",
    "            print(\"\\n===== Text Generation Model Training =====\")\n",
    "            print(\"1. Train basic LSTM model on a text file\")\n",
    "            print(\"2. Use Microsoft BitNet model\")\n",
    "            print(\"3. Load existing model and run interactive mode\")\n",
    "            print(\"4. Exit\")\n",
    "            \n",
    "            choice = input(\"\\nSelect an option (1-4): \")\n",
    "            \n",
    "            if choice == '1':\n",
    "                select_and_train()\n",
    "                input(\"\\nPress Enter to return to main menu...\")\n",
    "            \n",
    "            elif choice == '2':\n",
    "                # This would call the BitNet implementation\n",
    "                try:\n",
    "                    # Check if transformers is installed\n",
    "                    import importlib\n",
    "                    if importlib.util.find_spec(\"transformers\") is None:\n",
    "                        print(\"Installing required packages for BitNet...\")\n",
    "                        !pip install -q transformers accelerate datasets\n",
    "                    \n",
    "                    # BitNet functions in cell 4\n",
    "                    print(\"Loading BitNet functionality...\")\n",
    "                    use_bitnet()\n",
    "                    input(\"\\nPress Enter to return to main menu...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error using BitNet: {e}\")\n",
    "                    input(\"\\nPress Enter to return to main menu...\")\n",
    "            \n",
    "            elif choice == '3':\n",
    "                # List and load existing models\n",
    "                try:\n",
    "                    models_dir = 'models'\n",
    "                    if not os.path.exists(models_dir):\n",
    "                        os.makedirs(models_dir)\n",
    "                        print(\"Models directory created. Please train a model first.\")\n",
    "                        input(\"\\nPress Enter to return to main menu...\")\n",
    "                        continue\n",
    "                    \n",
    "                    models = [f for f in os.listdir(models_dir) if f.endswith('.pt')]\n",
    "                    if not models:\n",
    "                        print(\"No saved models found. Please train a model first.\")\n",
    "                        input(\"\\nPress Enter to return to main menu...\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(\"\\nAvailable models:\")\n",
    "                    for i, model_file in enumerate(models):\n",
    "                        print(f\"{i+1}. {model_file}\")\n",
    "                    \n",
    "                    idx_input = input(\"\\nSelect a model number: \")\n",
    "                    try:\n",
    "                        idx = int(idx_input) - 1\n",
    "                        if 0 <= idx < len(models):\n",
    "                            model_path = f\"{models_dir}/{models[idx]}\"\n",
    "                            model, chars, string_to_int, int_to_string, device = load_model(model_path)\n",
    "                            interactive_mode(model, string_to_int, int_to_string, device)\n",
    "                        else:\n",
    "                            print(\"Invalid selection.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Invalid input.\")\n",
    "                    \n",
    "                    input(\"\\nPress Enter to return to main menu...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading model: {e}\")\n",
    "                    input(\"\\nPress Enter to return to main menu...\")\n",
    "            \n",
    "            elif choice == '4':\n",
    "                print(\"Exiting program. Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                print(\"Invalid choice. Please enter a number between 1 and 4.\")\n",
    "                input(\"\\nPress Enter to try again...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in main menu: {e}\")\n",
    "            input(\"\\nPress Enter to try again...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc32947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: BitNet Integration\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class BitNetTextGenerator:\n",
    "    \"\"\"Wrapper class to integrate Microsoft BitNet with existing project\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"microsoft/bitnet-b1.58-2B-4T\", use_cache=True, cache_dir=\"./bitnet_cache\"):\n",
    "        \"\"\"Initialize BitNet model and tokenizer\"\"\"\n",
    "        print(f\"Loading BitNet model: {model_name}\")\n",
    "        \n",
    "        # Create cache directory if it doesn't exist\n",
    "        if use_cache and not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "            \n",
    "        # Set device\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name, \n",
    "                cache_dir=cache_dir if use_cache else None\n",
    "            )\n",
    "            \n",
    "            # Load model with optimizations\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=cache_dir if use_cache else None,\n",
    "                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n",
    "                device_map=\"auto\" if self.device == 'cuda' else None,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            # Connect with your character vocabulary\n",
    "            self.char_to_token = {}  # Will map your characters to token IDs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing BitNet: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def prepare_data_for_finetuning(self, text_file_path, max_length=256, stride=128):\n",
    "        \"\"\"Prepare data from your text file for BitNet fine-tuning\"\"\"\n",
    "        print(\"Preparing data for fine-tuning...\")\n",
    "        \n",
    "        try:\n",
    "            # Read your existing text file\n",
    "            with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Create char to token mapping if needed\n",
    "            if not self.char_to_token:\n",
    "                unique_chars = sorted(set(text))\n",
    "                for char in unique_chars:\n",
    "                    token_id = self.tokenizer.encode(char, add_special_tokens=False)\n",
    "                    if token_id:  # Some characters might not have a direct mapping\n",
    "                        self.char_to_token[char] = token_id[0]\n",
    "            \n",
    "            # Tokenize text - using stride to create overlapping examples for better learning\n",
    "            encodings = self.tokenizer(text, return_overflowing_tokens=True, \n",
    "                                      max_length=max_length, stride=stride)\n",
    "            \n",
    "            # Prepare dataset\n",
    "            examples = []\n",
    "            for i in range(len(encodings[\"input_ids\"])):\n",
    "                examples.append({\n",
    "                    \"input_ids\": torch.tensor(encodings[\"input_ids\"][i]),\n",
    "                    \"attention_mask\": torch.tensor(encodings[\"attention_mask\"][i]),\n",
    "                    \"labels\": torch.tensor(encodings[\"input_ids\"][i])  # For causal language modeling\n",
    "                })\n",
    "            \n",
    "            # Create Dataset and split\n",
    "            dataset = Dataset.from_list(examples)\n",
    "            dataset = dataset.train_test_split(test_size=0.1)\n",
    "            \n",
    "            print(f\"Created dataset with {len(dataset['train'])} training and {len(dataset['test'])} validation examples\")\n",
    "            return dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def finetune(self, dataset, output_dir=\"./models/bitnet-finetuned\", epochs=1, batch_size=2):\n",
    "        \"\"\"Fine-tune BitNet on your data\"\"\"\n",
    "        print(\"Starting BitNet fine-tuning...\")\n",
    "        \n",
    "        try:\n",
    "            # Create output directory if it doesn't exist\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Adjust batch size based on device\n",
    "            actual_batch_size = batch_size\n",
    "            if self.device == 'cpu':\n",
    "                actual_batch_size = 1\n",
    "                print(\"Running on CPU, reducing batch size to 1\")\n",
    "            \n",
    "            # Setup training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                overwrite_output_dir=True,\n",
    "                num_train_epochs=epochs,\n",
    "                per_device_train_batch_size=actual_batch_size,\n",
    "                per_device_eval_batch_size=actual_batch_size,\n",
    "                gradient_accumulation_steps=4,  # To simulate larger batch sizes\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=100,\n",
    "                save_steps=100,\n",
    "                save_total_limit=2,  # Only keep the 2 best checkpoints\n",
    "                learning_rate=5e-5,\n",
    "                fp16=self.device == 'cuda',\n",
    "                load_best_model_at_end=True,\n",
    "                logging_steps=50,\n",
    "                report_to=\"none\"  # Disable wandb reporting\n",
    "            )\n",
    "            \n",
    "            # Create trainer\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"test\"],\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            trainer.train()\n",
    "            \n",
    "            # Save the model and tokenizer\n",
    "            self.model.save_pretrained(output_dir)\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "            \n",
    "            print(f\"Model fine-tuned and saved to {output_dir}\")\n",
    "            return trainer\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during fine-tuning: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_text(self, prompt, max_new_tokens=100, temperature=0.8, top_p=0.9):\n",
    "        \"\"\"Generate text using BitNet\"\"\"\n",
    "        try:\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Handle empty prompt\n",
    "            if not prompt:\n",
    "                inputs = self.tokenizer(\"Hello\", return_tensors=\"pt\").to(self.device)\n",
    "            else:\n",
    "                # Encode the prompt\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            # Generate text\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return generated_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating text: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def interactive_mode(self):\n",
    "        \"\"\"Interactive mode compatible with your existing interface\"\"\"\n",
    "        print(\"\\n===== BitNet Text Generation =====\")\n",
    "        print(\"Type some text and press Enter to have the model continue it.\")\n",
    "        print(\"Type ':temp X' to set temperature (e.g., ':temp 0.5')\")\n",
    "        print(\"Type ':length X' to set generation length (e.g., ':length 200')\")\n",
    "        print(\"Type ':exit' to quit.\")\n",
    "        \n",
    "        temperature = 0.8\n",
    "        length = 100\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\n> \")\n",
    "                \n",
    "                # Handle commands\n",
    "                if user_input.startswith(':'):\n",
    "                    cmd = user_input.lower()\n",
    "                    if cmd == ':exit':\n",
    "                        break\n",
    "                    elif cmd.startswith(':temp '):\n",
    "                        try:\n",
    "                            temperature = float(cmd.split()[1])\n",
    "                            print(f\"Temperature set to {temperature}\")\n",
    "                        except:\n",
    "                            print(\"Invalid temperature. Format: :temp 0.8\")\n",
    "                    elif cmd.startswith(':length '):\n",
    "                        try:\n",
    "                            length = int(cmd.split()[1])\n",
    "                            print(f\"Generation length set to {length}\")\n",
    "                        except:\n",
    "                            print(\"Invalid length. Format: :length 100\")\n",
    "                    continue\n",
    "                \n",
    "                # Generate text\n",
    "                print(\"\\nGenerating...\")\n",
    "                generated_text = self.generate_text(user_input, max_new_tokens=length, temperature=temperature)\n",
    "                print(\"\\nOutput:\")\n",
    "                print(\"-\" * 40)\n",
    "                print(generated_text)\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nExiting interactive mode.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "def use_bitnet():\n",
    "    \"\"\"Function to use BitNet with your project\"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    \n",
    "    try:\n",
    "        # Initialize BitNet\n",
    "        bitnet = BitNetTextGenerator()\n",
    "        \n",
    "        # List available text files\n",
    "        files = [f for f in os.listdir('data') if f.endswith('.txt')]\n",
    "        \n",
    "        if not files:\n",
    "            print(\"No text files found in the 'data' directory.\")\n",
    "            print(\"Please add text files to the 'data' directory and try again.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nAvailable text files for BitNet:\")\n",
    "        for i, file in enumerate(files):\n",
    "            print(f\"{i+1}. {file}\")\n",
    "        \n",
    "        # Get user selection\n",
    "        choice = input(\"\\nSelect a file number (or press Enter for default sample2.txt): \")\n",
    "        if choice.strip() == '':\n",
    "            file_path = 'sample2.txt'\n",
    "        else:\n",
    "            try:\n",
    "                idx = int(choice) - 1\n",
    "                if 0 <= idx < len(files):\n",
    "                    file_path = f\"data/{files[idx]}\"\n",
    "                else:\n",
    "                    print(\"Invalid selection. Using default file.\")\n",
    "                    file_path = 'sample2.txt'\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Using default file.\")\n",
    "                file_path = 'sample2.txt'\n",
    "        \n",
    "        # Verify file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File {file_path} not found.\")\n",
    "            return\n",
    "        \n",
    "        # Ask about fine-tuning\n",
    "        fine_tune = input(\"Do you want to fine-tune BitNet on your data? (y/n): \").lower().startswith('y')\n",
    "        \n",
    "        if fine_tune:\n",
    "            # Get epochs\n",
    "            epochs_input = input(\"Enter number of epochs (default: 1): \")\n",
    "            epochs = int(epochs_input) if epochs_input.strip().isdigit() else 1\n",
    "            \n",
    "            # Prepare data\n",
    "            try:\n",
    "                dataset = bitnet.prepare_data_for_finetuning(file_path)\n",
    "                \n",
    "                # Fine-tune\n",
    "                output_dir = f\"./models/bitnet-{os.path.basename(file_path).split('.')[0]}\"\n",
    "                trainer = bitnet.finetune(dataset, output_dir=output_dir, epochs=epochs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during fine-tuning process: {e}\")\n",
    "                print(\"Continuing with pre-trained model...\")\n",
    "        \n",
    "        # Generate samples\n",
    "        print(\"\\nGenerating samples with BitNet:\")\n",
    "        for temp in [0.5, 0.8, 1.0]:\n",
    "            print(f\"\\nSample with temperature {temp}:\")\n",
    "            sample = bitnet.generate_text(\"\", max_new_tokens=100, temperature=temp)\n",
    "            print(f\"{sample}\\n{'-'*40}\")\n",
    "        \n",
    "        # Interactive mode\n",
    "        use_interactive = input(\"\\nWould you like to enter interactive mode? (y/n): \")\n",
    "        if use_interactive.lower().startswith('y'):\n",
    "            bitnet.interactive_mode()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error using BitNet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e976ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run the Main Menu\n",
    "try:\n",
    "    # This will start the application\n",
    "    main_menu()\n",
    "except Exception as e:\n",
    "    print(f\"Error running main menu: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
